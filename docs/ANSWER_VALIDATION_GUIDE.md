# Answer Validation Guide

How to verify if your RAG system's answers are correct.

---

## ğŸ” Method 1: Manual Chunk Verification (Most Reliable)

**Process:**
1. Ask a question
2. Check what chunks were retrieved (in API logs)
3. Read those chunks manually
4. Verify LLM answer matches chunk content

### Example:

**Question:** "What is Knowella?"

**API Logs Show:**
```
ğŸ” Retrieving chunks for: "What is Knowella?"
  1. About - Knowella (score: 0.892)
  2. Features - Knowella (score: 0.845)
  3. Pricing - Knowella (score: 0.803)
```

**Step 1: Get chunk content**
```bash
# Get the actual chunk that was retrieved
curl -s -X POST http://localhost:6333/collections/knowella_pages/points/scroll \
  -H "Content-Type: application/json" \
  -d '{"limit": 1, "with_payload": true, "with_vector": false, "filter": {"must": [{"key": "title", "match": {"text": "About - Knowella"}}]}}' \
  | jq '.result.points[0].payload.text'
```

**Step 2: Compare**
- âœ… **Correct:** LLM answer contains info from chunks
- âŒ **Wrong:** LLM answer has info NOT in chunks (hallucination)

---

## ğŸ“‹ Method 2: Ground Truth Dataset (Best for Testing)

Create a file with **known correct answers**:

### `test-dataset.json`
```json
[
  {
    "id": 1,
    "question": "What results do organizations see with Knowella?",
    "expected_facts": [
      "1.8x increase in engagement",
      "70% less data-entry time",
      "45% more actionable insights",
      "62% drop in workplace accidents"
    ],
    "must_include": ["1.8", "70%", "45%", "62%"],
    "must_not_include": ["made up fact", "hallucination"]
  },
  {
    "id": 2,
    "question": "What is AI ergonomics?",
    "expected_facts": [
      "Video analysis of posture",
      "RULA and REBA assessments",
      "Risk scoring",
      "Real-time feedback"
    ],
    "must_include": ["posture", "risk"],
    "must_not_include": []
  }
]
```

### **Validation Script:**
```javascript
// validate-answers.js
const testDataset = require('./test-dataset.json');

async function validateAnswer(question, llmAnswer, expected) {
  const results = {
    hasAllFacts: true,
    missingFacts: [],
    hallucinations: [],
    score: 0
  };

  // Check if all required keywords are present
  for (const keyword of expected.must_include) {
    if (!llmAnswer.toLowerCase().includes(keyword.toLowerCase())) {
      results.hasAllFacts = false;
      results.missingFacts.push(keyword);
    }
  }

  // Check for hallucinations
  for (const badKeyword of expected.must_not_include) {
    if (llmAnswer.toLowerCase().includes(badKeyword.toLowerCase())) {
      results.hallucinations.push(badKeyword);
    }
  }

  // Calculate score
  const totalKeywords = expected.must_include.length;
  const foundKeywords = totalKeywords - results.missingFacts.length;
  results.score = (foundKeywords / totalKeywords) * 100;

  return results;
}
```

---

## ğŸŒ Method 3: Check Source URLs (Quick Verification)

**Process:**
1. LLM provides answer with source URLs
2. Visit those URLs
3. Verify information exists on the page

### Example:

**LLM Answer:**
> "Knowella helps with supply chain management through real-time visibility, automated workflows, and AI-driven risk detection."
>
> Sources:
> - https://www.knowella.com/blog/best-practices-for-optimizing-inventory-and-logistics/

**Validation:**
1. âœ… Visit URL
2. âœ… Search page for "real-time visibility" â†’ Found!
3. âœ… Search for "automated workflows" â†’ Found!
4. âœ… Search for "AI-driven risk" â†’ Found!

**Result:** âœ… Answer is accurate

---

## ğŸ¤– Method 4: Fact Extraction & Verification

**Use this checklist for every answer:**

| Check | Question | Pass/Fail |
|-------|----------|-----------|
| **Grounding** | Is every fact in the answer present in the retrieved chunks? | âœ…/âŒ |
| **Completeness** | Does it answer the question fully? | âœ…/âŒ |
| **Accuracy** | Are numbers, names, and terms correct? | âœ…/âŒ |
| **No Hallucination** | Is there any made-up information? | âœ…/âŒ |
| **Citations** | Are source URLs relevant and real? | âœ…/âŒ |

### Example Validation:

**Question:** "What percentage improvements does Knowella report?"

**LLM Answer:** "Organizations using Knowella report 70% less data-entry time and 45% more actionable insights."

**Validation:**
```
âœ… Grounding: Check chunks â†’ "70% less data-entry time" found in chunk
âœ… Grounding: Check chunks â†’ "45% more actionable insights" found in chunk
âœ… Accuracy: Numbers are exact (70%, 45%)
âœ… No Hallucination: No made-up stats
âœ… Citations: Source URL provided
```

**Result:** âœ… Answer is correct

---

## ğŸ“Š Method 5: Automated Metrics

### **Option A: Semantic Similarity**

Compare LLM answer to ground truth using embeddings:

```javascript
async function calculateSimilarity(llmAnswer, groundTruth) {
  const embedding1 = await generateEmbedding(llmAnswer);
  const embedding2 = await generateEmbedding(groundTruth);

  // Cosine similarity
  const similarity = cosineSimilarity(embedding1, embedding2);

  return similarity; // 0.0 to 1.0
}

// Threshold
if (similarity > 0.85) {
  console.log('âœ… Answer is correct');
} else {
  console.log('âŒ Answer needs review');
}
```

### **Option B: Keyword Matching**

```javascript
function validateKeywords(answer, requiredKeywords) {
  const found = requiredKeywords.filter(kw =>
    answer.toLowerCase().includes(kw.toLowerCase())
  );

  const accuracy = (found.length / requiredKeywords.length) * 100;
  return accuracy; // 0 to 100%
}

// Example
const keywords = ['1.8x', '70%', '45%', '62%'];
const accuracy = validateKeywords(llmAnswer, keywords);

if (accuracy >= 75) {
  console.log('âœ… Answer is sufficiently accurate');
}
```

---

## ğŸ¯ Practical Testing Process

### **For Each Test Question:**

1. **Record the question**
2. **Save the LLM's answer**
3. **Check retrieved chunks** (from API logs)
4. **Manual verification:**
   - Read chunks
   - Highlight facts in chunks
   - Compare with LLM answer
   - Mark facts as âœ… (found) or âŒ (missing/wrong)

### **Example Test Record:**

```markdown
## Test #1: "What is Knowella?"

### LLM Answer:
"Knowella is a no-code operations management platform that helps teams digitize workflows, improve safety, and manage supply chains."

### Retrieved Chunks:
- Chunk 1: About - Knowella (score: 0.892)
- Chunk 2: Features - Knowella (0.845)

### Chunk Content (Chunk 1):
"Knowella is the all-in-one, frontline-first platform designed for operations, supply chain, and safety teams to streamline workflows, improve data visibility, and ensure compliance."

### Validation:
âœ… "no-code" â†’ Found in Chunk 1
âœ… "operations management" â†’ Found in Chunk 1
âœ… "digitize workflows" â†’ Found in Chunk 1
âœ… "improve safety" â†’ Found in Chunk 1
âœ… "supply chains" â†’ Found in Chunk 1

### Result: âœ… PASS (100% accurate)
```

---

## ğŸš¨ Red Flags (Hallucination Signs)

Watch for these in LLM answers:

| Red Flag | Example | Why It's Bad |
|----------|---------|--------------|
| **Specific numbers not in chunks** | "Knowella costs $99/month" (when pricing isn't in DB) | Made-up pricing |
| **Competitor mentions** | "Better than Asana or Monday" (if not in content) | Invented comparison |
| **Vague generalizations** | "Many experts say..." | Not grounded in content |
| **Outdated info** | References 2022 when content is 2026 | Wrong temporal context |
| **Made-up features** | "Knowella has blockchain integration" | Feature doesn't exist |

---

## ğŸ“ˆ Quality Scoring System

### **Per-Answer Score:**

```
Score = (Correct Facts / Total Facts) Ã— 100%

- 90-100%: âœ… Excellent (fully accurate)
- 75-89%: âš ï¸ Good (mostly accurate, minor issues)
- 50-74%: âŒ Poor (significant inaccuracies)
- <50%: âŒ Failed (mostly wrong or hallucinated)
```

### **Overall System Score:**

Test 20 questions, calculate average:

```
System Accuracy = Average of all answer scores

- 90%+: âœ… Production-ready
- 75-89%: âš ï¸ Needs improvement
- <75%: âŒ Not ready for production
```

---

## ğŸ› ï¸ Quick Validation Checklist

Copy this for each test:

```
Question: _______________________

LLM Answer:
_______________________

Retrieved Chunks:
1. _______________________
2. _______________________
3. _______________________

Validation:
[ ] Read all chunks manually
[ ] All facts in answer are in chunks
[ ] No hallucinations detected
[ ] Statistics/numbers are exact
[ ] Citations are correct
[ ] Answer is complete
[ ] Answer is concise

Score: __/7 checks passed

Grade:
[ ] âœ… Excellent (7/7)
[ ] âš ï¸ Good (5-6/7)
[ ] âŒ Poor (3-4/7)
[ ] âŒ Failed (<3/7)
```

---

## ğŸ’¡ Pro Tips

1. **Start with easy questions** - Build confidence in the system
2. **Test edge cases** - Questions about missing info, acronyms, stats
3. **Check citations** - Visit source URLs to verify
4. **Look for patterns** - If all pricing questions fail, there's a retrieval issue
5. **Document failures** - Record what went wrong and why

---

## ğŸ“ Example: Full Validation Flow

### Question: "What results do organizations see with Knowella?"

**Step 1: Get LLM answer**
```json
{
  "answer": "Organizations using Knowella report a 1.8x increase in engagement, 70% reduction in data-entry time, 45% more actionable insights, and 62% drop in workplace accidents.",
  "sources": [
    "https://www.knowella.com/blog/..."
  ]
}
```

**Step 2: Check API logs for chunks**
```
Retrieved chunks:
1. Top 10 Ways AI Transforms Supply Chain (score: 0.812)
```

**Step 3: Get chunk content**
```bash
curl ... | grep "1.8x\|70%\|45%\|62%"
```

**Output:**
```
"Organizations using Knowella have seen a 1.8Ã— increase in engagement, 70% less data-entry time, 45% more actionable insights, and a 62% drop in reportable workplace accidents"
```

**Step 4: Compare**
- âœ… "1.8x increase" â†’ Exact match in chunk
- âœ… "70% reduction" â†’ Found as "70% less" in chunk
- âœ… "45% more insights" â†’ Exact match
- âœ… "62% drop" â†’ Exact match

**Result:** âœ… **PASS - 100% Accurate**

---

## ğŸš€ Automated Testing Script

```javascript
// test-rag-accuracy.js
const questions = require('./test-dataset.json');

async function testRagAccuracy() {
  let totalScore = 0;
  let results = [];

  for (const test of questions) {
    // 1. Ask question
    const response = await askRAG(test.question);

    // 2. Get retrieved chunks
    const chunks = response.retrievedChunks;

    // 3. Validate answer
    const validation = await validateAnswer(
      response.answer,
      chunks,
      test.expected_facts
    );

    // 4. Record result
    results.push({
      question: test.question,
      answer: response.answer,
      validation: validation,
      passed: validation.score >= 75
    });

    totalScore += validation.score;
  }

  // Overall accuracy
  const avgAccuracy = totalScore / questions.length;

  console.log(`\nğŸ“Š Overall Accuracy: ${avgAccuracy.toFixed(1)}%`);
  console.log(`âœ… Passed: ${results.filter(r => r.passed).length}/${questions.length}`);

  return results;
}

testRagAccuracy();
```

---

**Bottom line:** The best validation is **reading the chunks yourself** and confirming the LLM's answer matches what's in those chunks. Start with that, then automate if needed! ğŸ¯
